# -*- coding: utf-8 -*-
"""Write_into_database.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aCaDUX7-LgspZoJLmfdQHh2dTvETbZMp
"""

! pip install bs4
! pip install lxml
import requests
import re
import pandas as pd
import time
import random
from datetime import date, timedelta
import pandas as pd
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup as soup
from urllib.parse import urljoin
from bs4 import BeautifulSoup as bs
from selenium.webdriver.common.by import By

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://archive.apache.org/dist/spark/spark-3.1.1/spark-3.1.1-bin-hadoop3.2.tgz
!tar xf spark-3.1.1-bin-hadoop3.2.tgz
!pip install -q findspark

# Set environment
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.1.1-bin-hadoop3.2"

import findspark
findspark.init()
from pyspark.sql import SparkSession

spark_jars = '/content/drive/MyDrive/Colab_Notebooks/postgresql-42.2.14.jar'

def get_spark(master="local[*]",name='DataEng'):
  builder = SparkSession.builder.appName(name)
  builder.config("spark.jars",spark_jars)
  return builder.getOrCreate()

spark = get_spark()
print(f'spark_session:{spark.version}')

# Import schema into the database
!psql -h dataeng.ctg03bab53x7.us-east-2.rds.amazonaws.com -d dataeng -U postgres -c '\i db.sql'

# read all the tables 
# Write all files into parquet format 
# Indicators
ccs = spark.read.parquet('/content/corruption-control.parquet')
ges=spark.read.parquet('/content/govern_effective.parquet')
pss=spark.read.parquet('/content/political_stab.parquet')
rls=spark.read.parquet('/content/rule_law.parquet')
rqs=spark.read.parquet('/content/reg_quality.parquet')
vas=spark.read.parquet('/content/voice_account.parquet')
indicator = spark.read.parquet('/content/Indicator')
# Corruption
corr= spark.read.parquet('/content/corruption')
# country
country = spark.read.parquet('/content/country_info.parquet')

url = 'jdbc:postgresql://dataeng.ctg03bab53x7.us-east-2.rds.amazonaws.com:5432/dataeng'
user = 'postgres'
password = 'Qwerty1234' 
driver = 'org.postgresql.Driver'
mode = 'append'

# Also import another connection method so that minor change can be made to adjust schema
import psycopg2
connection = psycopg2.connect(
    host = 'dataeng.ctg03bab53x7.us-east-2.rds.amazonaws.com',
    port = 5432,
    user = user,
    password = password,
    database= 'dataeng'
    )
cursor=connection.cursor()

# Insert parquet file into existing tables in the schema
ccs.write.jdbc(url=url,table='ccs_df',mode=mode,properties={'password':password,'user':user,'driver':driver})
ges.write.jdbc(url=url,table='ges_df',mode=mode,properties={'password':password,'user':user,'driver':driver})
pss.write.jdbc(url=url,table='pss_df',mode=mode,properties={'password':password,'user':user,'driver':driver})
rls.write.jdbc(url=url,table='rls_df',mode=mode,properties={'password':password,'user':user,'driver':driver})
rqs.write.jdbc(url=url,table='rgs_df',mode=mode,properties={'password':password,'user':user,'driver':driver})
vas.write.jdbc(url=url,table='vas_df',mode=mode,properties={'password':password,'user':user,'driver':driver})
corr.write.jdbc(url=url,table='corruption',mode=mode,properties={'password':password,'user':user,'driver':driver})
country.write.jdbc(url=url,table='country_info',mode=mode,properties={'password':password,'user':user,'driver':driver})
indicator.write.jdbc(url=url,table='Indicator',mode=mode,properties={'password':password,'user':user,'driver':driver})

# Match the year from corruption data from corruption table (i.e. from 2017 to 2022)
def match_year(df):
  year = range(2017,2023)
  df = df[df['year'].isin(year)]
  return df
corrupt_control_score = match_year(corrupt_control_score)
government_effectiveness_score= match_year(government_effectiveness_score)
regulation_quality_score = match_year(regulation_quality_score)
rule_law_score = match_year(rule_law_score)
voice_accountability_score=match_year(voice_accountability_score)
political_stab_score= match_year(political_stab_score)

# Merge to form Indicator table
merge_columns = ['country','country_code','year']
Indicator = pd.merge(political_stab_score,
                     pd.merge(voice_accountability_score,
                     pd.merge(rule_law_score,
                     pd.merge(regulation_quality_score,
                     pd.merge(corrupt_control_score,government_effectiveness_score, 
                              on=merge_columns),on=merge_columns),
                              on=merge_columns),on=merge_columns),on=merge_columns)

# Find the percentage change of each indidcator group by country
Indicator['corrupt_control_score_pct'] = Indicator.groupby('country_code')['corruption_control_score'].pct_change(periods=-1).mul(100).fillna(0).round(2)
Indicator['government_effectiveness_score_pct'] = Indicator.groupby('country_code')['government_effectiveness_score'].pct_change(periods=-1).mul(100).fillna(0).round(2)
Indicator['regulation_quality_score_pct'] = Indicator.groupby('country_code')['regulation_quality_score'].pct_change(periods=-1).mul(100).fillna(0).round(2)
Indicator['rule_law_score_pct'] = Indicator.groupby('country_code')['rule_law_score'].pct_change(periods=-1).mul(100).fillna(0).round(2)
Indicator['voice_accountability_score_pct'] = Indicator.groupby('country_code')['voice_accountability_score'].pct_change(periods=-1).mul(100).fillna(0).round(2)
Indicator['cpolitical_stab_score_pct'] = Indicator.groupby('country_code')['political_stab_score'].pct_change(periods=-1).mul(100).fillna(0).round(2)

# Convert all the dataframe into parquet file
ccs_df = spark.createDataFrame(corrupt_control_score)
ges_df = spark.createDataFrame(government_effectiveness_score)
rqs_df = spark.createDataFrame(regulation_quality_score)
rls_df = spark.createDataFrame(rule_law_score)
vas_df = spark.createDataFrame(voice_accountability_score)
pss_df = spark.createDataFrame(political_stab_score)
Indicator = spark.createDataFrame(Indicator)

# Change datatype
# Indicators
ccs_df.printSchema()
ges_df.printSchema()
rqs_df.printSchema()
rls_df.printSchema()
vas_df.printSchema()
pss_df.printSchema()
Indicator.printSchema()

corrupt_df = spark.read.parquet('/content/corruption').toPandas()

print(corrupt_df.info())
# change column types 
corrupt_df= corrupt_df.astype({
    'country':'str',
    'rank' : 'int',
    'year' : 'int',    
    'corrupt_score':'float'
})
print(corrupt_df.dtypes)
# Create a spark dataframe
corruption = spark.createDataFrame(corrupt_df)
corruption.printSchema()

# Write all files into parquet format 
# Indicators
ccs_df.write.parquet('corruption-control.parquet',mode='overwrite')
ges_df.write.parquet('govern_effective.parquet',mode='overwrite')
pss_df.write.parquet('political_stab.parquet',mode='overwrite')
rls_df.write.parquet('rule_law.parquet',mode='overwrite')
rqs_df.write.parquet('reg_quality.parquet',mode='overwrite')
vas_df.write.parquet('voice_account.parquet',mode='overwrite')
Indicator.write.parquet('Indicator',mode='overwrite')
# Corruption
corruption.write.parquet("corruption",mode='overwrite')